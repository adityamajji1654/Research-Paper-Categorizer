{"metadata":{"accelerator":"GPU","colab":{"name":"Fine-tuning BERT (and friends) for multi-label text classification.ipynb","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7591471,"sourceType":"datasetVersion","datasetId":4419051},{"sourceId":7591477,"sourceType":"datasetVersion","datasetId":4419057},{"sourceId":7591480,"sourceType":"datasetVersion","datasetId":4419059}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"091f8220f33241f288faa0612853585f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6111a73e684a47769bda7183a836ee91","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b1899a0c4b144d7a5e6599f8afb8b65","value":3}},"1045bb16e3694410898a73cf1b848917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbba60f793c14100934a268063f63d26","placeholder":"​","style":"IPY_MODEL_cd3570ddf67541d7818d97e236c54e54","value":" 3/3 [00:00&lt;00:00, 75.93it/s]"}},"308fa6a7348140ec981a8d6c7d31f346":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5080d322a8034924b652b379c04667ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6111a73e684a47769bda7183a836ee91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b1899a0c4b144d7a5e6599f8afb8b65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bc92587e35443488445e7521fbd0a13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5080d322a8034924b652b379c04667ed","placeholder":"​","style":"IPY_MODEL_cb95e545fbdd4e99903bf634df694c9f","value":"100%"}},"9a1aa9f2cc29473f9f8e5459d2641e76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bc92587e35443488445e7521fbd0a13","IPY_MODEL_091f8220f33241f288faa0612853585f","IPY_MODEL_1045bb16e3694410898a73cf1b848917"],"layout":"IPY_MODEL_308fa6a7348140ec981a8d6c7d31f346"}},"bbba60f793c14100934a268063f63d26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb95e545fbdd4e99903bf634df694c9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd3570ddf67541d7818d97e236c54e54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine-tuning BERT for multi-label text classification for research articles\n\nIn this notebook, we are going to fine-tune BERT to predict one or more labels for a given piece of text. Note that this notebook illustrates how to fine-tune a scibert_scivocab_uncased.\n\nNEED OF EDITS\nAll of those work in the same way: they add a linear layer on top of the base model, which is used to produce a tensor of shape (batch_size, num_labels), indicating the unnormalized scores for a number of labels for every example in the batch.\n\n\n","metadata":{"id":"kLB3I4FKZ5Lr"}},{"cell_type":"markdown","source":"## INSTALL LIBRARIES","metadata":{}},{"cell_type":"code","source":"# !pip install transformers torch nltk pandas scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-02-08T20:43:15.597108Z","iopub.execute_input":"2024-02-08T20:43:15.597488Z","iopub.status.idle":"2024-02-08T20:43:15.602022Z","shell.execute_reply.started":"2024-02-08T20:43:15.597455Z","shell.execute_reply":"2024-02-08T20:43:15.601132Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## MAKE NECESSARY IMPORTS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport gc\nimport random\nimport re\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Subset\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler,random_split\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import classification_report, f1_score\n\nimport transformers\nfrom transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:36:08.973288Z","iopub.execute_input":"2024-02-08T22:36:08.973643Z","iopub.status.idle":"2024-02-08T22:36:17.764949Z","shell.execute_reply.started":"2024-02-08T22:36:08.973611Z","shell.execute_reply":"2024-02-08T22:36:17.764121Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## PATH VARIABLES","metadata":{}},{"cell_type":"code","source":"TOKENIZER_SCIBERT_PATH=\"/kaggle/input/tokenizerscibert/\"\nMODEL_SCIBERT_PATH=\"/kaggle/input/modelscibert/\"\nTRAIN_DATASET_PATH=\"/kaggle/input/train-csv/train.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:38:59.108607Z","iopub.execute_input":"2024-02-08T22:38:59.108989Z","iopub.status.idle":"2024-02-08T22:38:59.113203Z","shell.execute_reply.started":"2024-02-08T22:38:59.108960Z","shell.execute_reply":"2024-02-08T22:38:59.112271Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## LOAD DATA\nload train data set and also required tokenizer and scibert model so we can use it offine\n","metadata":{"id":"bIH9NP0MZ6-O"}},{"cell_type":"code","source":"# Load Dataset\ndf = pd.read_csv(TRAIN_DATASET_PATH)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:39:19.449869Z","iopub.execute_input":"2024-02-08T22:39:19.450478Z","iopub.status.idle":"2024-02-08T22:39:20.842226Z","shell.execute_reply.started":"2024-02-08T22:39:19.450447Z","shell.execute_reply":"2024-02-08T22:39:20.841311Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      Id                                              Title  \\\n0   9707             Axiomatic Aspects of Default Inference   \n1  24198  On extensions of group with infinite conjugacy...   \n2  35766  An Analysis of Complex-Valued CNNs for RF Data...   \n3  14322  On the reconstruction of the drift of a diffus...   \n4    709  Three classes of propagation rules for GRS and...   \n\n                                            Abstract  \\\n0  This paper studies axioms for nonmonotonic con...   \n1  We characterize the group property of being wi...   \n2  Recent deep neural network-based device classi...   \n3  The problem of reconstructing the drift of a d...   \n4  In this paper, we study the Hermitian hulls of...   \n\n                                 Categories  \n0                                 ['cs.LO']  \n1                               ['math.GR']  \n2  ['cs.LG', 'cs.IT', 'eess.SP', 'math.IT']  \n3         ['math.PR', 'math.ST', 'stat.TH']  \n4                      ['cs.IT', 'math.IT']  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Title</th>\n      <th>Abstract</th>\n      <th>Categories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9707</td>\n      <td>Axiomatic Aspects of Default Inference</td>\n      <td>This paper studies axioms for nonmonotonic con...</td>\n      <td>['cs.LO']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24198</td>\n      <td>On extensions of group with infinite conjugacy...</td>\n      <td>We characterize the group property of being wi...</td>\n      <td>['math.GR']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35766</td>\n      <td>An Analysis of Complex-Valued CNNs for RF Data...</td>\n      <td>Recent deep neural network-based device classi...</td>\n      <td>['cs.LG', 'cs.IT', 'eess.SP', 'math.IT']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14322</td>\n      <td>On the reconstruction of the drift of a diffus...</td>\n      <td>The problem of reconstructing the drift of a d...</td>\n      <td>['math.PR', 'math.ST', 'stat.TH']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>709</td>\n      <td>Three classes of propagation rules for GRS and...</td>\n      <td>In this paper, we study the Hermitian hulls of...</td>\n      <td>['cs.IT', 'math.IT']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['text'] = df['Title'] +\" \"+ df['Abstract']\ndel df['Title']\ndel df['Abstract']\ndf['Categories'] = df['Categories'].str.replace(', ', ',')\ndf['Categories'] = df['Categories'].str.strip('[]')\ncategories_df = df['Categories'].str.get_dummies(sep=',')\ndf = pd.concat([df.drop('Categories', axis=1), categories_df], axis=1)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:39:39.411461Z","iopub.execute_input":"2024-02-08T22:39:39.411839Z","iopub.status.idle":"2024-02-08T22:39:41.500601Z","shell.execute_reply.started":"2024-02-08T22:39:39.411793Z","shell.execute_reply":"2024-02-08T22:39:41.499698Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"      Id                                               text  'cs.AI'  'cs.AR'  \\\n0   9707  Axiomatic Aspects of Default Inference This pa...        0        0   \n1  24198  On extensions of group with infinite conjugacy...        0        0   \n2  35766  An Analysis of Complex-Valued CNNs for RF Data...        0        0   \n3  14322  On the reconstruction of the drift of a diffus...        0        0   \n4    709  Three classes of propagation rules for GRS and...        0        0   \n\n   'cs.CE'  'cs.CL'  'cs.CR'  'cs.CV'  'cs.DB'  'cs.DC'  ...  'q-fin.MF'  \\\n0        0        0        0        0        0        0  ...           0   \n1        0        0        0        0        0        0  ...           0   \n2        0        0        0        0        0        0  ...           0   \n3        0        0        0        0        0        0  ...           0   \n4        0        0        0        0        0        0  ...           0   \n\n   'q-fin.PM'  'q-fin.PR'  'q-fin.RM'  'q-fin.TR'  'stat.AP'  'stat.CO'  \\\n0           0           0           0           0          0          0   \n1           0           0           0           0          0          0   \n2           0           0           0           0          0          0   \n3           0           0           0           0          0          0   \n4           0           0           0           0          0          0   \n\n   'stat.ME'  'stat.ML'  'stat.TH'  \n0          0          0          0  \n1          0          0          0  \n2          0          0          0  \n3          0          0          1  \n4          0          0          0  \n\n[5 rows x 59 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>text</th>\n      <th>'cs.AI'</th>\n      <th>'cs.AR'</th>\n      <th>'cs.CE'</th>\n      <th>'cs.CL'</th>\n      <th>'cs.CR'</th>\n      <th>'cs.CV'</th>\n      <th>'cs.DB'</th>\n      <th>'cs.DC'</th>\n      <th>...</th>\n      <th>'q-fin.MF'</th>\n      <th>'q-fin.PM'</th>\n      <th>'q-fin.PR'</th>\n      <th>'q-fin.RM'</th>\n      <th>'q-fin.TR'</th>\n      <th>'stat.AP'</th>\n      <th>'stat.CO'</th>\n      <th>'stat.ME'</th>\n      <th>'stat.ML'</th>\n      <th>'stat.TH'</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9707</td>\n      <td>Axiomatic Aspects of Default Inference This pa...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24198</td>\n      <td>On extensions of group with infinite conjugacy...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35766</td>\n      <td>An Analysis of Complex-Valued CNNs for RF Data...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14322</td>\n      <td>On the reconstruction of the drift of a diffus...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>709</td>\n      <td>Three classes of propagation rules for GRS and...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 59 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n\ndef clean_text(text):\n    \n    text = text.lower()\n    \n    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    #text = re.sub(r\"http\", \"\",text)\n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split()]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:39:48.800045Z","iopub.execute_input":"2024-02-08T22:39:48.800989Z","iopub.status.idle":"2024-02-08T22:39:48.807295Z","shell.execute_reply.started":"2024-02-08T22:39:48.800952Z","shell.execute_reply":"2024-02-08T22:39:48.806381Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:40:01.338684Z","iopub.execute_input":"2024-02-08T22:40:01.339060Z","iopub.status.idle":"2024-02-08T22:40:08.103751Z","shell.execute_reply.started":"2024-02-08T22:40:01.339031Z","shell.execute_reply":"2024-02-08T22:40:08.102700Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.columns = df.columns.str.strip(\"'\")\nnew_column_name = 'title_summary'  \ndf = df.rename(columns={df.columns[1]: new_column_name})\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:40:16.792064Z","iopub.execute_input":"2024-02-08T22:40:16.792417Z","iopub.status.idle":"2024-02-08T22:40:16.842960Z","shell.execute_reply.started":"2024-02-08T22:40:16.792388Z","shell.execute_reply":"2024-02-08T22:40:16.842006Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      Id                                      title_summary  cs.AI  cs.AR  \\\n0   9707  axiomatic aspects of default inference this pa...      0      0   \n1  24198  on extensions of group with infinite conjugacy...      0      0   \n2  35766  an analysis of complex valued cnns for rf data...      0      0   \n3  14322  on the reconstruction of the drift of a diffus...      0      0   \n4    709  three classes of propagation rules for grs and...      0      0   \n\n   cs.CE  cs.CL  cs.CR  cs.CV  cs.DB  cs.DC  ...  q-fin.MF  q-fin.PM  \\\n0      0      0      0      0      0      0  ...         0         0   \n1      0      0      0      0      0      0  ...         0         0   \n2      0      0      0      0      0      0  ...         0         0   \n3      0      0      0      0      0      0  ...         0         0   \n4      0      0      0      0      0      0  ...         0         0   \n\n   q-fin.PR  q-fin.RM  q-fin.TR  stat.AP  stat.CO  stat.ME  stat.ML  stat.TH  \n0         0         0         0        0        0        0        0        0  \n1         0         0         0        0        0        0        0        0  \n2         0         0         0        0        0        0        0        0  \n3         0         0         0        0        0        0        0        1  \n4         0         0         0        0        0        0        0        0  \n\n[5 rows x 59 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>title_summary</th>\n      <th>cs.AI</th>\n      <th>cs.AR</th>\n      <th>cs.CE</th>\n      <th>cs.CL</th>\n      <th>cs.CR</th>\n      <th>cs.CV</th>\n      <th>cs.DB</th>\n      <th>cs.DC</th>\n      <th>...</th>\n      <th>q-fin.MF</th>\n      <th>q-fin.PM</th>\n      <th>q-fin.PR</th>\n      <th>q-fin.RM</th>\n      <th>q-fin.TR</th>\n      <th>stat.AP</th>\n      <th>stat.CO</th>\n      <th>stat.ME</th>\n      <th>stat.ML</th>\n      <th>stat.TH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9707</td>\n      <td>axiomatic aspects of default inference this pa...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24198</td>\n      <td>on extensions of group with infinite conjugacy...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35766</td>\n      <td>an analysis of complex valued cnns for rf data...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14322</td>\n      <td>on the reconstruction of the drift of a diffus...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>709</td>\n      <td>three classes of propagation rules for grs and...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 59 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing.","metadata":{"id":"QCL02vQgxYTO"}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:42:59.213284Z","iopub.execute_input":"2024-02-08T22:42:59.213640Z","iopub.status.idle":"2024-02-08T22:42:59.241418Z","shell.execute_reply.started":"2024-02-08T22:42:59.213613Z","shell.execute_reply":"2024-02-08T22:42:59.240364Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 512\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 3\nLEARNING_RATE = 2e-5\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SCIBERT_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:09.965548Z","iopub.execute_input":"2024-02-08T22:43:09.966315Z","iopub.status.idle":"2024-02-08T22:43:10.022389Z","shell.execute_reply.started":"2024-02-08T22:43:09.966284Z","shell.execute_reply":"2024-02-08T22:43:10.021431Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"target_cols = [col for col in df.columns if col not in ['Id', 'title_summary']]\ntarget_cols","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:16.842365Z","iopub.execute_input":"2024-02-08T22:43:16.842719Z","iopub.status.idle":"2024-02-08T22:43:16.850497Z","shell.execute_reply.started":"2024-02-08T22:43:16.842691Z","shell.execute_reply":"2024-02-08T22:43:16.849529Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['cs.AI',\n 'cs.AR',\n 'cs.CE',\n 'cs.CL',\n 'cs.CR',\n 'cs.CV',\n 'cs.DB',\n 'cs.DC',\n 'cs.DM',\n 'cs.GT',\n 'cs.IR',\n 'cs.IT',\n 'cs.LG',\n 'cs.LO',\n 'cs.NI',\n 'cs.OS',\n 'cs.PL',\n 'cs.RO',\n 'cs.SD',\n 'cs.SE',\n 'econ.EM',\n 'econ.GN',\n 'econ.TH',\n 'eess.AS',\n 'eess.IV',\n 'eess.SP',\n 'math.AC',\n 'math.AP',\n 'math.AT',\n 'math.CO',\n 'math.CV',\n 'math.GR',\n 'math.IT',\n 'math.LO',\n 'math.NT',\n 'math.PR',\n 'math.QA',\n 'math.ST',\n 'q-bio.BM',\n 'q-bio.CB',\n 'q-bio.GN',\n 'q-bio.MN',\n 'q-bio.NC',\n 'q-bio.TO',\n 'q-fin.CP',\n 'q-fin.EC',\n 'q-fin.GN',\n 'q-fin.MF',\n 'q-fin.PM',\n 'q-fin.PR',\n 'q-fin.RM',\n 'q-fin.TR',\n 'stat.AP',\n 'stat.CO',\n 'stat.ME',\n 'stat.ML',\n 'stat.TH']"},"metadata":{}}]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.text = df.title_summary\n        self.tokenizer = tokenizer\n        self.targets = df[target_cols].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:23.987045Z","iopub.execute_input":"2024-02-08T22:43:23.987701Z","iopub.status.idle":"2024-02-08T22:43:23.996490Z","shell.execute_reply.started":"2024-02-08T22:43:23.987669Z","shell.execute_reply":"2024-02-08T22:43:23.995439Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df[target_cols].values","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:31.628577Z","iopub.execute_input":"2024-02-08T22:43:31.628934Z","iopub.status.idle":"2024-02-08T22:43:31.644761Z","shell.execute_reply.started":"2024-02-08T22:43:31.628907Z","shell.execute_reply":"2024-02-08T22:43:31.643462Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = BERTDataset(df, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:37.696901Z","iopub.execute_input":"2024-02-08T22:43:37.697255Z","iopub.status.idle":"2024-02-08T22:43:37.709627Z","shell.execute_reply.started":"2024-02-08T22:43:37.697218Z","shell.execute_reply":"2024-02-08T22:43:37.708481Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.roberta = model = AutoModel.from_pretrained(MODEL_SCIBERT_PATH)\n#         self.l2 = torch.nn.Dropout(0.3)\n        self.fc = torch.nn.Linear(768,57)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, features = self.roberta(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n#         output_2 = self.l2(output_1)\n        output = self.fc(features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:43.044718Z","iopub.execute_input":"2024-02-08T22:43:43.045381Z","iopub.status.idle":"2024-02-08T22:43:43.051244Z","shell.execute_reply.started":"2024-02-08T22:43:43.045348Z","shell.execute_reply":"2024-02-08T22:43:43.050322Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:51.048561Z","iopub.execute_input":"2024-02-08T22:43:51.049193Z","iopub.status.idle":"2024-02-08T22:43:51.053538Z","shell.execute_reply.started":"2024-02-08T22:43:51.049161Z","shell.execute_reply":"2024-02-08T22:43:51.052510Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def find_best_threshold(y_true, y_pred_probs):\n    best_threshold = 0.0\n    best_f1_score = 0.0\n    \n    for threshold in np.arange(-2.0, 2.0, 0.05):\n        y_pred = (y_pred_probs >= threshold).astype(int)\n        f1 = f1_score(y_true, y_pred, average='macro')\n        \n        if f1 > best_f1_score:\n            best_f1_score = f1\n            best_threshold = threshold\n            \n    return best_threshold, best_f1_score","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:43:57.117020Z","iopub.execute_input":"2024-02-08T22:43:57.117506Z","iopub.status.idle":"2024-02-08T22:43:57.123667Z","shell.execute_reply.started":"2024-02-08T22:43:57.117473Z","shell.execute_reply":"2024-02-08T22:43:57.122578Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    kf = KFold(n_splits=3, shuffle=True, random_state = 42)\n    results = []\n    final_threshold = 0\n    \n    for fold, (train_ids, val_ids) in enumerate(kf.split(train_dataset)):\n        \n        model = BERTClass()\n        model.to(device)\n        optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n        train_subs = Subset(train_dataset, train_ids)\n        val_subs = Subset(train_dataset, val_ids)\n        train_loader = DataLoader(train_subs, batch_size= TRAIN_BATCH_SIZE, shuffle=True)\n        validation_loader = DataLoader(val_subs, batch_size= VALID_BATCH_SIZE, shuffle=False)\n\n        model.train()\n        for i in range(epoch):\n            for j, data in enumerate(train_loader):\n                ids = data['ids'].to(device, dtype = torch.long)\n                mask = data['mask'].to(device, dtype = torch.long)\n                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n                targets = data['targets'].to(device, dtype = torch.float)\n                outputs = model(ids, mask, token_type_ids)\n                loss = loss_fn(outputs, targets)\n                if j%50 == 0:\n                    print(f'Epoch: {i}, batch : {j} Loss:  {loss.item()}')\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n        model.eval()\n        predictions = []\n        targets = []\n        with torch.no_grad():\n            for i, data in enumerate(validation_loader):\n                ids = data['ids'].to(device, dtype = torch.long)\n                mask = data['mask'].to(device, dtype = torch.long)\n                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n                targets.extend(data['targets'])\n                outputs = model(ids, mask, token_type_ids)\n                batch_predictions = outputs.squeeze().tolist()\n                predictions.extend(batch_predictions)\n            pred = np.array(predictions)\n            threshold, f1_score = find_best_threshold(targets, pred)\n            final_threshold += threshold/3\n            results.append(f1_score)\n        print(f\"f1_score for fold {fold} : {f1_score}\")\n        \n    model = BERTClass()\n    model.to(device)\n    optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n    train_loader = DataLoader(train_dataset, batch_size= TRAIN_BATCH_SIZE, shuffle=True)\n    model.train()\n    for i in range(epoch):\n        for j, data in enumerate(train_loader):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n\n            outputs = model(ids, mask, token_type_ids)\n\n            loss = loss_fn(outputs, targets)\n            if j%50 == 0:\n                print(f'Epoch: {i}, batch : {j} Loss:  {loss.item()}')\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n    return model, results, final_threshold","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:44:03.903136Z","iopub.execute_input":"2024-02-08T22:44:03.903881Z","iopub.status.idle":"2024-02-08T22:44:03.922200Z","shell.execute_reply.started":"2024-02-08T22:44:03.903850Z","shell.execute_reply":"2024-02-08T22:44:03.921099Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"epoch=3","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:44:13.808737Z","iopub.execute_input":"2024-02-08T22:44:13.809105Z","iopub.status.idle":"2024-02-08T22:44:13.813558Z","shell.execute_reply.started":"2024-02-08T22:44:13.809077Z","shell.execute_reply":"2024-02-08T22:44:13.812497Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model, results, final_threshold = train(epoch)\ntorch.save(model.state_dict(), 'model.bin')","metadata":{"execution":{"iopub.status.busy":"2024-02-08T22:47:41.242514Z","iopub.execute_input":"2024-02-08T22:47:41.242947Z","iopub.status.idle":"2024-02-09T05:48:27.400293Z","shell.execute_reply.started":"2024-02-08T22:47:41.242915Z","shell.execute_reply":"2024-02-09T05:48:27.399078Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch: 0, batch : 0 Loss:  0.6948950290679932\nEpoch: 0, batch : 50 Loss:  0.2813558876514435\nEpoch: 0, batch : 100 Loss:  0.19365939497947693\nEpoch: 0, batch : 150 Loss:  0.14521071314811707\nEpoch: 0, batch : 200 Loss:  0.15278439223766327\nEpoch: 0, batch : 250 Loss:  0.13711890578269958\nEpoch: 0, batch : 300 Loss:  0.11356864124536514\nEpoch: 0, batch : 350 Loss:  0.1287909597158432\nEpoch: 0, batch : 400 Loss:  0.11305023729801178\nEpoch: 0, batch : 450 Loss:  0.11815514415502548\nEpoch: 0, batch : 500 Loss:  0.0955762192606926\nEpoch: 0, batch : 550 Loss:  0.10760829597711563\nEpoch: 0, batch : 600 Loss:  0.11473220586776733\nEpoch: 0, batch : 650 Loss:  0.09339188784360886\nEpoch: 0, batch : 1350 Loss:  0.06779580563306808\nEpoch: 0, batch : 1400 Loss:  0.09142578393220901\nEpoch: 0, batch : 1450 Loss:  0.07584300637245178\nEpoch: 0, batch : 1500 Loss:  0.07085160911083221\nEpoch: 0, batch : 1550 Loss:  0.06282248347997665\nEpoch: 0, batch : 1600 Loss:  0.04811670258641243\nEpoch: 0, batch : 1650 Loss:  0.05200612545013428\nEpoch: 0, batch : 1700 Loss:  0.05683319643139839\nEpoch: 0, batch : 1750 Loss:  0.064064621925354\nEpoch: 0, batch : 1800 Loss:  0.06517602503299713\nEpoch: 0, batch : 1850 Loss:  0.07300210744142532\nEpoch: 0, batch : 1900 Loss:  0.070493683218956\nEpoch: 0, batch : 1950 Loss:  0.06012200191617012\nEpoch: 0, batch : 2000 Loss:  0.08088133484125137\nEpoch: 0, batch : 2050 Loss:  0.05424811691045761\nEpoch: 0, batch : 2100 Loss:  0.05618149787187576\nEpoch: 1, batch : 0 Loss:  0.05950043722987175\nEpoch: 1, batch : 50 Loss:  0.059771765023469925\nEpoch: 1, batch : 100 Loss:  0.05216551572084427\nEpoch: 1, batch : 150 Loss:  0.07320777326822281\nEpoch: 1, batch : 200 Loss:  0.04911031201481819\nEpoch: 1, batch : 250 Loss:  0.05040910840034485\nEpoch: 1, batch : 300 Loss:  0.05367009714245796\nEpoch: 1, batch : 350 Loss:  0.06660013645887375\nEpoch: 1, batch : 400 Loss:  0.06615615636110306\nEpoch: 1, batch : 450 Loss:  0.04332100600004196\nEpoch: 1, batch : 500 Loss:  0.06112745776772499\nEpoch: 1, batch : 550 Loss:  0.04717884212732315\nEpoch: 1, batch : 600 Loss:  0.06131049618124962\nEpoch: 1, batch : 650 Loss:  0.04752390831708908\nEpoch: 1, batch : 700 Loss:  0.06411382555961609\nEpoch: 1, batch : 750 Loss:  0.05015292018651962\nEpoch: 1, batch : 800 Loss:  0.055862363427877426\nEpoch: 1, batch : 850 Loss:  0.04568590968847275\nEpoch: 1, batch : 900 Loss:  0.03133025020360947\nEpoch: 1, batch : 950 Loss:  0.05088590085506439\nEpoch: 1, batch : 1000 Loss:  0.04357597976922989\nEpoch: 1, batch : 1050 Loss:  0.048903048038482666\nEpoch: 1, batch : 1100 Loss:  0.06763321906328201\nEpoch: 1, batch : 1150 Loss:  0.07553521543741226\nEpoch: 1, batch : 1200 Loss:  0.04435458034276962\nEpoch: 1, batch : 1250 Loss:  0.056940820068120956\nEpoch: 1, batch : 1300 Loss:  0.03545454517006874\nEpoch: 1, batch : 1350 Loss:  0.05895499140024185\nEpoch: 1, batch : 1400 Loss:  0.07023327052593231\nEpoch: 1, batch : 1450 Loss:  0.04433482885360718\nEpoch: 1, batch : 1500 Loss:  0.0720549151301384\nEpoch: 1, batch : 1550 Loss:  0.03992411121726036\nEpoch: 1, batch : 1600 Loss:  0.04564318060874939\nEpoch: 1, batch : 1650 Loss:  0.047139402478933334\nEpoch: 1, batch : 1700 Loss:  0.031170746311545372\nEpoch: 1, batch : 1750 Loss:  0.047424472868442535\nEpoch: 1, batch : 1800 Loss:  0.050404757261276245\nEpoch: 1, batch : 1850 Loss:  0.0458529032766819\nEpoch: 1, batch : 1900 Loss:  0.06152956187725067\nEpoch: 1, batch : 1950 Loss:  0.06074153259396553\nEpoch: 1, batch : 2000 Loss:  0.05939698591828346\nEpoch: 1, batch : 2050 Loss:  0.03295458108186722\nEpoch: 1, batch : 2100 Loss:  0.05241335928440094\nEpoch: 2, batch : 0 Loss:  0.018615880981087685\nEpoch: 2, batch : 50 Loss:  0.03679319843649864\nEpoch: 2, batch : 100 Loss:  0.031132446601986885\nEpoch: 2, batch : 150 Loss:  0.05174444615840912\nEpoch: 2, batch : 200 Loss:  0.029216879978775978\nEpoch: 2, batch : 250 Loss:  0.0441112257540226\nEpoch: 2, batch : 300 Loss:  0.044599372893571854\nEpoch: 2, batch : 350 Loss:  0.048433564603328705\nEpoch: 2, batch : 400 Loss:  0.051988452672958374\nEpoch: 2, batch : 450 Loss:  0.05024703964591026\nEpoch: 2, batch : 500 Loss:  0.041630107909440994\nEpoch: 2, batch : 550 Loss:  0.03880884125828743\nEpoch: 2, batch : 600 Loss:  0.05107169598340988\nEpoch: 2, batch : 650 Loss:  0.04880278930068016\nEpoch: 2, batch : 700 Loss:  0.04558931291103363\nEpoch: 2, batch : 750 Loss:  0.05640598386526108\nEpoch: 2, batch : 800 Loss:  0.042287956923246384\nEpoch: 2, batch : 850 Loss:  0.03382771089673042\nEpoch: 2, batch : 900 Loss:  0.04800911992788315\nEpoch: 2, batch : 950 Loss:  0.04310128465294838\nEpoch: 2, batch : 1000 Loss:  0.04377242922782898\nEpoch: 2, batch : 1050 Loss:  0.038892678916454315\nEpoch: 2, batch : 1100 Loss:  0.04542546719312668\nEpoch: 2, batch : 1150 Loss:  0.04025144502520561\nEpoch: 2, batch : 1200 Loss:  0.045179758220911026\nEpoch: 2, batch : 1250 Loss:  0.029692821204662323\nEpoch: 2, batch : 1300 Loss:  0.04144580289721489\nEpoch: 2, batch : 1350 Loss:  0.045212291181087494\nEpoch: 2, batch : 1400 Loss:  0.01982499100267887\nEpoch: 2, batch : 1450 Loss:  0.031360700726509094\nEpoch: 2, batch : 1500 Loss:  0.03941042348742485\nEpoch: 2, batch : 1550 Loss:  0.0330672562122345\nEpoch: 2, batch : 1600 Loss:  0.04786614328622818\nEpoch: 2, batch : 1650 Loss:  0.025795340538024902\nEpoch: 2, batch : 1700 Loss:  0.039765164256095886\nEpoch: 2, batch : 1750 Loss:  0.033381279557943344\nEpoch: 2, batch : 1800 Loss:  0.021099254488945007\nEpoch: 2, batch : 1850 Loss:  0.04683738574385643\nEpoch: 2, batch : 1900 Loss:  0.06722082942724228\nEpoch: 2, batch : 1950 Loss:  0.05117771774530411\nEpoch: 2, batch : 2000 Loss:  0.039141539484262466\nEpoch: 2, batch : 2050 Loss:  0.01572207175195217\nEpoch: 2, batch : 2100 Loss:  0.03324928134679794\nf1_score for fold 0 : 0.6687139526017833\nEpoch: 0, batch : 0 Loss:  0.7102501392364502\nEpoch: 0, batch : 50 Loss:  0.289795458316803\nEpoch: 0, batch : 100 Loss:  0.20176270604133606\nEpoch: 0, batch : 150 Loss:  0.15603232383728027\nEpoch: 0, batch : 200 Loss:  0.16595283150672913\nEpoch: 0, batch : 250 Loss:  0.13443627953529358\nEpoch: 0, batch : 300 Loss:  0.15388894081115723\nEpoch: 0, batch : 350 Loss:  0.11824473738670349\nEpoch: 0, batch : 400 Loss:  0.10485861450433731\nEpoch: 0, batch : 450 Loss:  0.1169733852148056\nEpoch: 0, batch : 500 Loss:  0.10450760275125504\nEpoch: 0, batch : 550 Loss:  0.10499755293130875\nEpoch: 0, batch : 600 Loss:  0.09793274849653244\nEpoch: 0, batch : 650 Loss:  0.09511161595582962\nEpoch: 0, batch : 700 Loss:  0.08738403022289276\nEpoch: 0, batch : 750 Loss:  0.10631806403398514\nEpoch: 0, batch : 800 Loss:  0.09330291301012039\nEpoch: 0, batch : 850 Loss:  0.08969301730394363\nEpoch: 0, batch : 900 Loss:  0.1024770438671112\nEpoch: 0, batch : 950 Loss:  0.08720647543668747\nEpoch: 0, batch : 1000 Loss:  0.06960935145616531\nEpoch: 0, batch : 1050 Loss:  0.08328519016504288\nEpoch: 0, batch : 1100 Loss:  0.07721598446369171\nEpoch: 0, batch : 1150 Loss:  0.05959687754511833\nEpoch: 0, batch : 1200 Loss:  0.08749475330114365\nEpoch: 0, batch : 1250 Loss:  0.06977201998233795\nEpoch: 0, batch : 1300 Loss:  0.07756549119949341\nEpoch: 0, batch : 1350 Loss:  0.09314210712909698\nEpoch: 0, batch : 1400 Loss:  0.08192279189825058\nEpoch: 0, batch : 1450 Loss:  0.07408683747053146\nEpoch: 0, batch : 1500 Loss:  0.07354123145341873\nEpoch: 0, batch : 1550 Loss:  0.0669315904378891\nEpoch: 0, batch : 1600 Loss:  0.07094892114400864\nEpoch: 0, batch : 1650 Loss:  0.06610002368688583\nEpoch: 0, batch : 1700 Loss:  0.07627497613430023\nEpoch: 0, batch : 1750 Loss:  0.0600663423538208\nEpoch: 0, batch : 1800 Loss:  0.06826764345169067\nEpoch: 0, batch : 1850 Loss:  0.05560661479830742\nEpoch: 0, batch : 1900 Loss:  0.05516761541366577\nEpoch: 0, batch : 1950 Loss:  0.06723551452159882\nEpoch: 0, batch : 2000 Loss:  0.06043724715709686\nEpoch: 0, batch : 2050 Loss:  0.06021827086806297\nEpoch: 0, batch : 2100 Loss:  0.06650778651237488\nEpoch: 1, batch : 0 Loss:  0.06788153946399689\nEpoch: 1, batch : 50 Loss:  0.04815198853611946\nEpoch: 1, batch : 100 Loss:  0.06354934722185135\nEpoch: 1, batch : 150 Loss:  0.06313593685626984\nEpoch: 1, batch : 200 Loss:  0.05259401351213455\nEpoch: 1, batch : 250 Loss:  0.0512336865067482\nEpoch: 1, batch : 300 Loss:  0.0611862987279892\nEpoch: 1, batch : 350 Loss:  0.04892302304506302\nEpoch: 1, batch : 400 Loss:  0.052811238914728165\nEpoch: 1, batch : 450 Loss:  0.055601444095373154\nEpoch: 1, batch : 500 Loss:  0.04578898102045059\nEpoch: 1, batch : 550 Loss:  0.03319907188415527\nEpoch: 1, batch : 600 Loss:  0.04375462979078293\nEpoch: 1, batch : 650 Loss:  0.05906875431537628\nEpoch: 1, batch : 700 Loss:  0.051814090460538864\nEpoch: 1, batch : 750 Loss:  0.0552692711353302\nEpoch: 1, batch : 800 Loss:  0.04719723016023636\nEpoch: 1, batch : 850 Loss:  0.0415700264275074\nEpoch: 1, batch : 900 Loss:  0.055580686777830124\nEpoch: 1, batch : 950 Loss:  0.047276563942432404\nEpoch: 1, batch : 1000 Loss:  0.0481269396841526\nEpoch: 1, batch : 1050 Loss:  0.04827471077442169\nEpoch: 1, batch : 1100 Loss:  0.054903239011764526\nEpoch: 1, batch : 1150 Loss:  0.062174733728170395\nEpoch: 1, batch : 1200 Loss:  0.06620825827121735\nEpoch: 1, batch : 1250 Loss:  0.05696205794811249\nEpoch: 1, batch : 1300 Loss:  0.03653581812977791\nEpoch: 1, batch : 1350 Loss:  0.04920126125216484\nEpoch: 1, batch : 1400 Loss:  0.04606715217232704\nEpoch: 1, batch : 1450 Loss:  0.054966650903224945\nEpoch: 1, batch : 1500 Loss:  0.05603665113449097\nEpoch: 1, batch : 1550 Loss:  0.0678640827536583\nEpoch: 1, batch : 1600 Loss:  0.03518413379788399\nEpoch: 1, batch : 1650 Loss:  0.052034299820661545\nEpoch: 1, batch : 1700 Loss:  0.06984610855579376\nEpoch: 1, batch : 1750 Loss:  0.03577767685055733\nEpoch: 1, batch : 1800 Loss:  0.06659571826457977\nEpoch: 1, batch : 1850 Loss:  0.05938296765089035\nEpoch: 1, batch : 1900 Loss:  0.039892490953207016\nEpoch: 1, batch : 1950 Loss:  0.056406766176223755\nEpoch: 1, batch : 2000 Loss:  0.04659644141793251\nEpoch: 1, batch : 2050 Loss:  0.04745639115571976\nEpoch: 1, batch : 2100 Loss:  0.043161001056432724\nEpoch: 2, batch : 0 Loss:  0.05567888915538788\nEpoch: 2, batch : 50 Loss:  0.04559609293937683\nEpoch: 2, batch : 100 Loss:  0.057293955236673355\nEpoch: 2, batch : 150 Loss:  0.02502761408686638\nEpoch: 2, batch : 200 Loss:  0.05829383805394173\nEpoch: 2, batch : 250 Loss:  0.05962119996547699\nEpoch: 2, batch : 300 Loss:  0.046194639056921005\nEpoch: 2, batch : 350 Loss:  0.02359415404498577\nEpoch: 2, batch : 400 Loss:  0.03200380504131317\nEpoch: 2, batch : 450 Loss:  0.03773825615644455\nEpoch: 2, batch : 500 Loss:  0.04304543882608414\nEpoch: 2, batch : 550 Loss:  0.03400082886219025\nEpoch: 2, batch : 600 Loss:  0.03777775913476944\nEpoch: 2, batch : 650 Loss:  0.04477258026599884\nEpoch: 2, batch : 700 Loss:  0.03901882842183113\nEpoch: 2, batch : 750 Loss:  0.04158863052725792\nEpoch: 2, batch : 800 Loss:  0.04313969612121582\nEpoch: 2, batch : 850 Loss:  0.03757241740822792\nEpoch: 2, batch : 900 Loss:  0.038435012102127075\nEpoch: 2, batch : 950 Loss:  0.05318786948919296\nEpoch: 2, batch : 1000 Loss:  0.044688500463962555\nEpoch: 2, batch : 1050 Loss:  0.051165781915187836\nEpoch: 2, batch : 1100 Loss:  0.029741061851382256\nEpoch: 2, batch : 1150 Loss:  0.04924853518605232\nEpoch: 2, batch : 1200 Loss:  0.03183629736304283\nEpoch: 2, batch : 1250 Loss:  0.03291662409901619\nEpoch: 2, batch : 1300 Loss:  0.02355288714170456\nEpoch: 2, batch : 1350 Loss:  0.03511263057589531\nEpoch: 2, batch : 1400 Loss:  0.0317167267203331\nEpoch: 2, batch : 1450 Loss:  0.038397081196308136\nEpoch: 2, batch : 1500 Loss:  0.05857930704951286\nEpoch: 2, batch : 1550 Loss:  0.03651612624526024\nEpoch: 2, batch : 1600 Loss:  0.04248441383242607\nEpoch: 2, batch : 1650 Loss:  0.023126838728785515\nEpoch: 2, batch : 1700 Loss:  0.05632086843252182\nEpoch: 2, batch : 1750 Loss:  0.03435727208852768\nEpoch: 2, batch : 1800 Loss:  0.037531524896621704\nEpoch: 2, batch : 1850 Loss:  0.04838050156831741\nEpoch: 2, batch : 1900 Loss:  0.03933320194482803\nEpoch: 2, batch : 1950 Loss:  0.04142289608716965\nEpoch: 2, batch : 2000 Loss:  0.057331737130880356\nEpoch: 2, batch : 2050 Loss:  0.03389083221554756\nEpoch: 2, batch : 2100 Loss:  0.04380572587251663\nf1_score for fold 1 : 0.6534781469319657\nEpoch: 0, batch : 0 Loss:  0.6728230118751526\nEpoch: 0, batch : 50 Loss:  0.2905757427215576\nEpoch: 0, batch : 100 Loss:  0.1746928095817566\nEpoch: 0, batch : 150 Loss:  0.16288964450359344\nEpoch: 0, batch : 200 Loss:  0.16720320284366608\nEpoch: 0, batch : 250 Loss:  0.13861992955207825\nEpoch: 0, batch : 300 Loss:  0.137043297290802\nEpoch: 0, batch : 350 Loss:  0.12628424167633057\nEpoch: 0, batch : 400 Loss:  0.11456461250782013\nEpoch: 0, batch : 450 Loss:  0.12594500184059143\nEpoch: 0, batch : 500 Loss:  0.12844233214855194\nEpoch: 0, batch : 550 Loss:  0.10302393138408661\nEpoch: 0, batch : 600 Loss:  0.10913602262735367\nEpoch: 0, batch : 650 Loss:  0.08322890102863312\nEpoch: 0, batch : 700 Loss:  0.08823533356189728\nEpoch: 0, batch : 750 Loss:  0.07720687985420227\nEpoch: 0, batch : 800 Loss:  0.09595576673746109\nEpoch: 0, batch : 850 Loss:  0.07652533799409866\nEpoch: 0, batch : 900 Loss:  0.08094163239002228\nEpoch: 0, batch : 950 Loss:  0.08175640553236008\nEpoch: 0, batch : 1000 Loss:  0.08008241653442383\nEpoch: 0, batch : 1050 Loss:  0.08458749949932098\nEpoch: 0, batch : 1100 Loss:  0.07398174703121185\nEpoch: 0, batch : 1150 Loss:  0.10703977197408676\nEpoch: 0, batch : 1200 Loss:  0.069330595433712\nEpoch: 0, batch : 1250 Loss:  0.08338722586631775\nEpoch: 0, batch : 1300 Loss:  0.05834020674228668\nEpoch: 0, batch : 1350 Loss:  0.07998505234718323\nEpoch: 0, batch : 1400 Loss:  0.08053480088710785\nEpoch: 0, batch : 1450 Loss:  0.06711892783641815\nEpoch: 0, batch : 1500 Loss:  0.07908831536769867\nEpoch: 0, batch : 1550 Loss:  0.06700963526964188\nEpoch: 0, batch : 1600 Loss:  0.0641346201300621\nEpoch: 0, batch : 1650 Loss:  0.0671827495098114\nEpoch: 0, batch : 1700 Loss:  0.06556610763072968\nEpoch: 0, batch : 1750 Loss:  0.07576382905244827\nEpoch: 0, batch : 1800 Loss:  0.08114893734455109\nEpoch: 0, batch : 1850 Loss:  0.05845418944954872\nEpoch: 0, batch : 1900 Loss:  0.05957517772912979\nEpoch: 0, batch : 1950 Loss:  0.056036122143268585\nEpoch: 0, batch : 2000 Loss:  0.05664638429880142\nEpoch: 0, batch : 2050 Loss:  0.05955537036061287\nEpoch: 0, batch : 2100 Loss:  0.04862065985798836\nEpoch: 1, batch : 0 Loss:  0.07162012159824371\nEpoch: 1, batch : 50 Loss:  0.04834622144699097\nEpoch: 1, batch : 100 Loss:  0.053184155374765396\nEpoch: 1, batch : 150 Loss:  0.062031250447034836\nEpoch: 1, batch : 200 Loss:  0.03877938538789749\nEpoch: 1, batch : 250 Loss:  0.058958012610673904\nEpoch: 1, batch : 300 Loss:  0.058484405279159546\nEpoch: 1, batch : 350 Loss:  0.05118795111775398\nEpoch: 1, batch : 400 Loss:  0.05680377781391144\nEpoch: 1, batch : 450 Loss:  0.06778974086046219\nEpoch: 1, batch : 500 Loss:  0.06455658376216888\nEpoch: 1, batch : 550 Loss:  0.04550068452954292\nEpoch: 1, batch : 600 Loss:  0.06802069395780563\nEpoch: 1, batch : 650 Loss:  0.043337397277355194\nEpoch: 1, batch : 700 Loss:  0.05056218430399895\nEpoch: 1, batch : 750 Loss:  0.06376731395721436\nEpoch: 1, batch : 800 Loss:  0.04690665379166603\nEpoch: 1, batch : 850 Loss:  0.05053238570690155\nEpoch: 1, batch : 900 Loss:  0.050894010812044144\nEpoch: 1, batch : 950 Loss:  0.04283757135272026\nEpoch: 1, batch : 1000 Loss:  0.04213598743081093\nEpoch: 1, batch : 1050 Loss:  0.061379324644804\nEpoch: 1, batch : 1100 Loss:  0.06386620551347733\nEpoch: 1, batch : 1150 Loss:  0.0669272169470787\nEpoch: 1, batch : 1200 Loss:  0.06133795157074928\nEpoch: 1, batch : 1250 Loss:  0.04759902134537697\nEpoch: 1, batch : 1300 Loss:  0.053106121718883514\nEpoch: 1, batch : 1350 Loss:  0.042228080332279205\nEpoch: 1, batch : 1400 Loss:  0.03948695957660675\nEpoch: 1, batch : 1450 Loss:  0.04871439188718796\nEpoch: 1, batch : 1500 Loss:  0.05484980344772339\nEpoch: 1, batch : 1550 Loss:  0.04895251244306564\nEpoch: 1, batch : 1600 Loss:  0.0635288879275322\nEpoch: 1, batch : 1650 Loss:  0.052417002618312836\nEpoch: 1, batch : 1700 Loss:  0.05614378675818443\nEpoch: 1, batch : 1750 Loss:  0.0488961786031723\nEpoch: 1, batch : 1800 Loss:  0.04093606024980545\nEpoch: 1, batch : 1850 Loss:  0.05489242821931839\nEpoch: 1, batch : 1900 Loss:  0.049557361751794815\nEpoch: 1, batch : 1950 Loss:  0.06575680524110794\nEpoch: 1, batch : 2000 Loss:  0.02883131615817547\nEpoch: 1, batch : 2050 Loss:  0.062181152403354645\nEpoch: 1, batch : 2100 Loss:  0.054515160620212555\nEpoch: 2, batch : 0 Loss:  0.029134541749954224\nEpoch: 2, batch : 50 Loss:  0.030204806476831436\nEpoch: 2, batch : 100 Loss:  0.03328502178192139\nEpoch: 2, batch : 150 Loss:  0.04162988439202309\nEpoch: 2, batch : 200 Loss:  0.03810770437121391\nEpoch: 2, batch : 250 Loss:  0.05424770712852478\nEpoch: 2, batch : 300 Loss:  0.04138796031475067\nEpoch: 2, batch : 350 Loss:  0.042687468230724335\nEpoch: 2, batch : 400 Loss:  0.03161207586526871\nEpoch: 2, batch : 450 Loss:  0.05003756284713745\nEpoch: 2, batch : 500 Loss:  0.0500885508954525\nEpoch: 2, batch : 550 Loss:  0.03894863277673721\nEpoch: 2, batch : 600 Loss:  0.03905753791332245\nEpoch: 2, batch : 650 Loss:  0.03539951145648956\nEpoch: 2, batch : 700 Loss:  0.029265088960528374\nEpoch: 2, batch : 750 Loss:  0.03744260594248772\nEpoch: 2, batch : 800 Loss:  0.04603743925690651\nEpoch: 2, batch : 850 Loss:  0.05058024823665619\nEpoch: 2, batch : 900 Loss:  0.025806983932852745\nEpoch: 2, batch : 950 Loss:  0.032197561115026474\nEpoch: 2, batch : 1000 Loss:  0.04638858512043953\nEpoch: 2, batch : 1050 Loss:  0.03461182862520218\nEpoch: 2, batch : 1100 Loss:  0.02720257267355919\nEpoch: 2, batch : 1150 Loss:  0.03199528530240059\nEpoch: 2, batch : 1200 Loss:  0.03253694251179695\nEpoch: 2, batch : 1250 Loss:  0.03726360574364662\nEpoch: 2, batch : 1300 Loss:  0.05241915583610535\nEpoch: 2, batch : 1350 Loss:  0.04299576207995415\nEpoch: 2, batch : 1400 Loss:  0.036890529096126556\nEpoch: 2, batch : 1450 Loss:  0.028696788474917412\nEpoch: 2, batch : 1500 Loss:  0.03877760097384453\nEpoch: 2, batch : 1550 Loss:  0.034435756504535675\nEpoch: 2, batch : 1600 Loss:  0.03884473443031311\nEpoch: 2, batch : 1650 Loss:  0.05188471078872681\nEpoch: 2, batch : 1700 Loss:  0.0442543625831604\nEpoch: 2, batch : 1750 Loss:  0.03303331509232521\nEpoch: 2, batch : 1800 Loss:  0.03547249361872673\nEpoch: 2, batch : 1850 Loss:  0.03690777346491814\nEpoch: 2, batch : 1900 Loss:  0.06113940104842186\nEpoch: 2, batch : 1950 Loss:  0.045698851346969604\nEpoch: 2, batch : 2000 Loss:  0.06586942821741104\nEpoch: 2, batch : 2050 Loss:  0.053481925278902054\nEpoch: 2, batch : 2100 Loss:  0.058816827833652496\nf1_score for fold 2 : 0.6645800427910645\nEpoch: 0, batch : 0 Loss:  0.7039647698402405\nEpoch: 0, batch : 50 Loss:  0.2966945767402649\nEpoch: 0, batch : 100 Loss:  0.17997035384178162\nEpoch: 0, batch : 150 Loss:  0.17863835394382477\nEpoch: 0, batch : 200 Loss:  0.14995959401130676\nEpoch: 0, batch : 250 Loss:  0.14100325107574463\nEpoch: 0, batch : 300 Loss:  0.12648950517177582\nEpoch: 0, batch : 350 Loss:  0.11051557958126068\nEpoch: 0, batch : 400 Loss:  0.12769126892089844\nEpoch: 0, batch : 450 Loss:  0.12054859101772308\nEpoch: 0, batch : 500 Loss:  0.1156763881444931\nEpoch: 0, batch : 550 Loss:  0.11294689774513245\nEpoch: 0, batch : 600 Loss:  0.09872046858072281\nEpoch: 0, batch : 650 Loss:  0.12430251389741898\nEpoch: 0, batch : 700 Loss:  0.10271494835615158\nEpoch: 0, batch : 750 Loss:  0.09173581004142761\nEpoch: 0, batch : 800 Loss:  0.08242421597242355\nEpoch: 0, batch : 850 Loss:  0.09311135858297348\nEpoch: 0, batch : 900 Loss:  0.08372177928686142\nEpoch: 0, batch : 950 Loss:  0.0954928994178772\nEpoch: 0, batch : 1000 Loss:  0.07944836467504501\nEpoch: 0, batch : 1050 Loss:  0.09901399165391922\nEpoch: 0, batch : 1100 Loss:  0.06770376861095428\nEpoch: 0, batch : 1150 Loss:  0.07220018655061722\nEpoch: 0, batch : 1200 Loss:  0.07936687022447586\nEpoch: 0, batch : 1250 Loss:  0.09208479523658752\nEpoch: 0, batch : 1300 Loss:  0.07147093117237091\nEpoch: 0, batch : 1350 Loss:  0.04764177277684212\nEpoch: 0, batch : 1400 Loss:  0.0654674768447876\nEpoch: 0, batch : 1450 Loss:  0.07985270768404007\nEpoch: 0, batch : 1500 Loss:  0.061956021934747696\nEpoch: 0, batch : 1550 Loss:  0.0654669851064682\nEpoch: 0, batch : 1600 Loss:  0.04719413071870804\nEpoch: 0, batch : 1650 Loss:  0.049088772386312485\nEpoch: 0, batch : 1700 Loss:  0.05299043655395508\nEpoch: 0, batch : 1750 Loss:  0.06512729823589325\nEpoch: 0, batch : 1800 Loss:  0.04884900897741318\nEpoch: 0, batch : 1850 Loss:  0.06081780418753624\nEpoch: 0, batch : 1900 Loss:  0.0583006888628006\nEpoch: 0, batch : 1950 Loss:  0.07142538577318192\nEpoch: 0, batch : 2000 Loss:  0.05013662204146385\nEpoch: 0, batch : 2050 Loss:  0.0891823098063469\nEpoch: 0, batch : 2100 Loss:  0.0811186209321022\nEpoch: 0, batch : 2150 Loss:  0.09107986092567444\nEpoch: 0, batch : 2200 Loss:  0.08117900788784027\nEpoch: 0, batch : 2250 Loss:  0.057043176144361496\nEpoch: 0, batch : 2300 Loss:  0.06091737747192383\nEpoch: 0, batch : 2350 Loss:  0.05190543830394745\nEpoch: 0, batch : 2400 Loss:  0.05110905319452286\nEpoch: 0, batch : 2450 Loss:  0.05252109840512276\nEpoch: 0, batch : 2500 Loss:  0.05182214081287384\nEpoch: 0, batch : 2550 Loss:  0.06245923414826393\nEpoch: 0, batch : 2600 Loss:  0.04673241078853607\nEpoch: 0, batch : 2650 Loss:  0.06097012013196945\nEpoch: 0, batch : 2700 Loss:  0.05862307921051979\nEpoch: 0, batch : 2750 Loss:  0.07524289935827255\nEpoch: 0, batch : 2800 Loss:  0.06707979738712311\nEpoch: 0, batch : 2850 Loss:  0.06288793683052063\nEpoch: 0, batch : 2900 Loss:  0.07076527923345566\nEpoch: 0, batch : 2950 Loss:  0.049953777343034744\nEpoch: 0, batch : 3000 Loss:  0.048553261905908585\nEpoch: 0, batch : 3050 Loss:  0.06598527729511261\nEpoch: 0, batch : 3100 Loss:  0.058319393545389175\nEpoch: 0, batch : 3150 Loss:  0.06633436679840088\nEpoch: 0, batch : 3200 Loss:  0.03934832289814949\nEpoch: 1, batch : 0 Loss:  0.06088835000991821\nEpoch: 1, batch : 50 Loss:  0.057895343750715256\nEpoch: 1, batch : 100 Loss:  0.046217180788517\nEpoch: 1, batch : 150 Loss:  0.05640292912721634\nEpoch: 1, batch : 200 Loss:  0.050683699548244476\nEpoch: 1, batch : 250 Loss:  0.044900521636009216\nEpoch: 1, batch : 300 Loss:  0.05928550660610199\nEpoch: 1, batch : 350 Loss:  0.04517200589179993\nEpoch: 1, batch : 400 Loss:  0.06137923523783684\nEpoch: 1, batch : 450 Loss:  0.046730250120162964\nEpoch: 1, batch : 500 Loss:  0.03945431858301163\nEpoch: 1, batch : 550 Loss:  0.04767673835158348\nEpoch: 1, batch : 600 Loss:  0.05159250274300575\nEpoch: 1, batch : 650 Loss:  0.05146621912717819\nEpoch: 1, batch : 700 Loss:  0.06405886262655258\nEpoch: 1, batch : 750 Loss:  0.05300644785165787\nEpoch: 1, batch : 800 Loss:  0.03867604583501816\nEpoch: 1, batch : 850 Loss:  0.08212440460920334\nEpoch: 1, batch : 900 Loss:  0.04930644482374191\nEpoch: 1, batch : 950 Loss:  0.038280829787254333\nEpoch: 1, batch : 1000 Loss:  0.05372677370905876\nEpoch: 1, batch : 1050 Loss:  0.04217749461531639\nEpoch: 1, batch : 1100 Loss:  0.044622790068387985\nEpoch: 1, batch : 1150 Loss:  0.04955234006047249\nEpoch: 1, batch : 1200 Loss:  0.041568223387002945\nEpoch: 1, batch : 1250 Loss:  0.06559579819440842\nEpoch: 1, batch : 1300 Loss:  0.05628272145986557\nEpoch: 1, batch : 1350 Loss:  0.04871780052781105\nEpoch: 1, batch : 1400 Loss:  0.05087971314787865\nEpoch: 1, batch : 1450 Loss:  0.04973500221967697\nEpoch: 1, batch : 1500 Loss:  0.04746657982468605\nEpoch: 1, batch : 1550 Loss:  0.0477476492524147\nEpoch: 1, batch : 1600 Loss:  0.043894506990909576\nEpoch: 1, batch : 1650 Loss:  0.04589875414967537\nEpoch: 1, batch : 1700 Loss:  0.04546898230910301\nEpoch: 1, batch : 1750 Loss:  0.05401281639933586\nEpoch: 1, batch : 1800 Loss:  0.04846133664250374\nEpoch: 1, batch : 1850 Loss:  0.039969660341739655\nEpoch: 1, batch : 1900 Loss:  0.046274151653051376\nEpoch: 1, batch : 1950 Loss:  0.03726758435368538\nEpoch: 1, batch : 2000 Loss:  0.040828634053468704\nEpoch: 1, batch : 2050 Loss:  0.03815227001905441\nEpoch: 1, batch : 2100 Loss:  0.04461468383669853\nEpoch: 1, batch : 2150 Loss:  0.039947040379047394\nEpoch: 1, batch : 2200 Loss:  0.055328454822301865\nEpoch: 1, batch : 2250 Loss:  0.05811276286840439\nEpoch: 1, batch : 2300 Loss:  0.032056376338005066\nEpoch: 1, batch : 2350 Loss:  0.051049668341875076\nEpoch: 1, batch : 2400 Loss:  0.038065627217292786\nEpoch: 1, batch : 2450 Loss:  0.04646102711558342\nEpoch: 1, batch : 2500 Loss:  0.047717779874801636\nEpoch: 1, batch : 2550 Loss:  0.04135880246758461\nEpoch: 1, batch : 2600 Loss:  0.045571502298116684\nEpoch: 1, batch : 2650 Loss:  0.02309785597026348\nEpoch: 1, batch : 2700 Loss:  0.05525878816843033\nEpoch: 1, batch : 2750 Loss:  0.04700036346912384\nEpoch: 1, batch : 2800 Loss:  0.05094820633530617\nEpoch: 1, batch : 2850 Loss:  0.035781558603048325\nEpoch: 1, batch : 2900 Loss:  0.045991718769073486\nEpoch: 1, batch : 2950 Loss:  0.038701023906469345\nEpoch: 1, batch : 3000 Loss:  0.05920683592557907\nEpoch: 1, batch : 3050 Loss:  0.04439384490251541\nEpoch: 1, batch : 3100 Loss:  0.041546352207660675\nEpoch: 1, batch : 3150 Loss:  0.04632467404007912\nEpoch: 1, batch : 3200 Loss:  0.03798779100179672\nEpoch: 2, batch : 0 Loss:  0.04683873802423477\nEpoch: 2, batch : 50 Loss:  0.03421994671225548\nEpoch: 2, batch : 100 Loss:  0.02842426858842373\nEpoch: 2, batch : 150 Loss:  0.028755012899637222\nEpoch: 2, batch : 200 Loss:  0.04571729898452759\nEpoch: 2, batch : 250 Loss:  0.030565675348043442\nEpoch: 2, batch : 300 Loss:  0.06098334863781929\nEpoch: 2, batch : 350 Loss:  0.03634076938033104\nEpoch: 2, batch : 400 Loss:  0.04076988995075226\nEpoch: 2, batch : 500 Loss:  0.027679577469825745\nEpoch: 2, batch : 550 Loss:  0.0316464826464653\nEpoch: 2, batch : 600 Loss:  0.025208057835698128\nEpoch: 2, batch : 650 Loss:  0.02561108209192753\nEpoch: 2, batch : 700 Loss:  0.03049960546195507\nEpoch: 2, batch : 750 Loss:  0.057175371795892715\nEpoch: 2, batch : 800 Loss:  0.03744465112686157\nEpoch: 2, batch : 850 Loss:  0.045522402971982956\nEpoch: 2, batch : 900 Loss:  0.03140009939670563\nEpoch: 2, batch : 950 Loss:  0.05116177722811699\nEpoch: 2, batch : 1000 Loss:  0.033977970480918884\nEpoch: 2, batch : 1050 Loss:  0.0421602800488472\nEpoch: 2, batch : 1100 Loss:  0.03525042161345482\nEpoch: 2, batch : 1150 Loss:  0.04179995134472847\nEpoch: 2, batch : 1200 Loss:  0.03270331025123596\nEpoch: 2, batch : 1250 Loss:  0.030767571181058884\nEpoch: 2, batch : 1350 Loss:  0.03859679028391838\nEpoch: 2, batch : 1400 Loss:  0.03369883820414543\nEpoch: 2, batch : 1450 Loss:  0.037437304854393005\nEpoch: 2, batch : 1500 Loss:  0.030969534069299698\nEpoch: 2, batch : 1550 Loss:  0.046953655779361725\nEpoch: 2, batch : 1600 Loss:  0.03010953590273857\nEpoch: 2, batch : 2250 Loss:  0.0471746101975441\nEpoch: 2, batch : 2300 Loss:  0.036183785647153854\nEpoch: 2, batch : 2350 Loss:  0.03984116390347481\nEpoch: 2, batch : 2400 Loss:  0.03472421318292618\nEpoch: 2, batch : 2450 Loss:  0.03279295936226845\nEpoch: 2, batch : 2500 Loss:  0.04309629276394844\nEpoch: 2, batch : 2550 Loss:  0.027532000094652176\nEpoch: 2, batch : 2600 Loss:  0.040105368942022324\nEpoch: 2, batch : 2650 Loss:  0.03529435768723488\nEpoch: 2, batch : 2700 Loss:  0.03939471021294594\nEpoch: 2, batch : 2750 Loss:  0.01985163800418377\nEpoch: 2, batch : 2800 Loss:  0.03185870498418808\nEpoch: 2, batch : 2850 Loss:  0.039114292711019516\nEpoch: 2, batch : 2900 Loss:  0.033164747059345245\nEpoch: 2, batch : 2950 Loss:  0.03724110499024391\nEpoch: 2, batch : 3000 Loss:  0.02949649840593338\nEpoch: 2, batch : 3050 Loss:  0.03965252265334129\nEpoch: 2, batch : 3100 Loss:  0.028806636109948158\nEpoch: 2, batch : 3150 Loss:  0.05162122845649719\nEpoch: 2, batch : 3200 Loss:  0.04042154178023338\n","output_type":"stream"}]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-02-09T06:01:47.977219Z","iopub.execute_input":"2024-02-09T06:01:47.977585Z","iopub.status.idle":"2024-02-09T06:01:47.983677Z","shell.execute_reply.started":"2024-02-09T06:01:47.977559Z","shell.execute_reply":"2024-02-09T06:01:47.982806Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[0.6687139526017833, 0.6534781469319657, 0.6645800427910645]"},"metadata":{}}]},{"cell_type":"code","source":"final_threshold","metadata":{"execution":{"iopub.status.busy":"2024-02-09T06:16:53.198721Z","iopub.execute_input":"2024-02-09T06:16:53.199544Z","iopub.status.idle":"2024-02-09T06:16:53.205146Z","shell.execute_reply.started":"2024-02-09T06:16:53.199514Z","shell.execute_reply":"2024-02-09T06:16:53.204179Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"-1.0499999999999992"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}