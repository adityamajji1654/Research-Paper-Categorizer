{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4wxY3x-ZZz8h","trusted":true},"outputs":[],"source":["# !pip install -q transformers datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["9a1aa9f2cc29473f9f8e5459d2641e76","308fa6a7348140ec981a8d6c7d31f346","8bc92587e35443488445e7521fbd0a13","091f8220f33241f288faa0612853585f","1045bb16e3694410898a73cf1b848917","cb95e545fbdd4e99903bf634df694c9f","5080d322a8034924b652b379c04667ed","8b1899a0c4b144d7a5e6599f8afb8b65","6111a73e684a47769bda7183a836ee91","cd3570ddf67541d7818d97e236c54e54","bbba60f793c14100934a268063f63d26"]},"id":"sd1LiXGjZ420","outputId":"1b5783cd-0e4e-4c92-c67c-57b9288f2381","trusted":true},"outputs":[],"source":["# from datasets import load_dataset\n","\n","# dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import time\n","import datetime\n","import gc\n","import random\n","from nltk.corpus import stopwords\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n","\n","import transformers\n","from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"../input/train-csv/train.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['text'] = df['Title'] +\" \"+ df['Abstract']\n","del df['Title']\n","del df['Abstract']\n","df['Categories'] = df['Categories'].str.replace(', ', ',')\n","df['Categories'] = df['Categories'].str.strip('[]')\n","categories_df = df['Categories'].str.get_dummies(sep=',')\n","df = pd.concat([df.drop('Categories', axis=1), categories_df], axis=1)\n","df.head()\n","# column_names_list = df.columns.tolist()\n","# print(column_names_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sw = stopwords.words('english')\n","\n","def clean_text(text):\n","    \n","    text = text.lower()\n","    \n","    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","\n","    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n","    #text = re.sub(r\"http\", \"\",text)\n","    \n","    html=re.compile(r'<.*?>') \n","    \n","    text = html.sub(r'',text) #Removing html tags\n","    \n","    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n","    for p in punctuations:\n","        text = text.replace(p,'') #Removing punctuations\n","        \n","    text = [word.lower() for word in text.split()]\n","    \n","    text = \" \".join(text) #removing stopwords\n","    \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['text'] = df['text'].apply(lambda x: clean_text(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.columns = df.columns.str.strip(\"'\")\n","new_column_name = 'title_summary'  # Specify the new name you want\n","\n","# Use the rename method to change the column name\n","df = df.rename(columns={df.columns[1]: new_column_name})\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"QCL02vQgxYTO"},"source":["As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target_cols = [\"math.AT\", \"stat.AP\", \"cs.AR\", \"math.QA\", \"q-bio.MN\", \"eess.AS\", \"eess.IV\", \"stat.ME\", \"econ.GN\", \"eess.SP\", \n","               \"q-fin.RM\", \"cs.LG\", \"cs.CR\", \"q-bio.BM\", \"q-fin.GN\", \"q-fin.MF\", \"q-fin.PR\", \"math.CV\", \"cs.LO\", \"econ.TH\", \n","               \"math.CO\", \"cs.AI\", \"math.AC\", \"q-bio.CB\", \"q-fin.CP\", \"cs.CL\", \"cs.DC\", \"math.LO\", \"math.NT\", \"cs.SD\", \n","               \"q-fin.TR\", \"cs.CV\", \"stat.ML\", \"q-fin.EC\", \"econ.EM\", \"cs.CE\", \"stat.CO\", \"math.PR\", \"q-bio.NC\", \"math.AP\", \n","               \"cs.OS\", \"cs.NI\", \"cs.IT\", \"cs.PL\", \"cs.GT\", \"cs.DM\", \"math.IT\", \"cs.SE\", \"cs.RO\", \"stat.TH\", \"cs.DB\", \n","               \"math.ST\", \"q-bio.GN\", \"q-fin.PM\", \"q-bio.TO\", \"math.GR\", \"cs.IR\"]\n","prefixes = {}\n","\n","for col in target_cols:\n","    prefix, sub_cat = col.split(\".\")\n","    if prefix not in prefixes:\n","        prefixes[prefix]=[]\n","    prefixes[prefix].append(col)\n","\n","print(prefixes)\n","\n","\n","def get_domain(row, x):\n","    sum_d= row[x].sum()\n","    if sum_d>0:\n","        return 1\n","    else:\n","        return 0\n","\n","for x in prefixes:\n","    df[x] = df.apply(get_domain, args=(prefixes[x],), axis=1)\n","   \n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE = 2e-5\n","tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# label_columns = df.columns[2:].tolist()\n","# df[label_columns] = df[label_columns].astype('float')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["level1_cols=[x for x in prefixes]\n","print(level1_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["level2_cols = [col for col in df.columns if col not in ['Id', 'title_summary']+level1_cols]\n","level2_cols\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BERTDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_len, level1_cols, level2_cols):\n","        self.df = df\n","        self.max_len = max_len\n","        self.text = df.title_summary\n","        self.tokenizer = tokenizer\n","        self.targets_level1 = df[level1_cols].values  # Targets for level 1\n","        self.targets_level2 = df[level2_cols].values  # Targets for level 2\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        text = self.text[index]\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            truncation=True,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets_level1': torch.tensor(self.targets_level1[index], dtype=torch.float),\n","            'targets_level2': torch.tensor(self.targets_level2[index], dtype=torch.float)\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df[target_cols].values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = BERTDataset(df, tokenizer, MAX_LEN,level1_cols,level2_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# next(iter(train_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, \n","                          num_workers=4, shuffle=True, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BERTClass(torch.nn.Module):\n","    def __init__(self, num_labels_level1, num_labels_level2):\n","        super(BERTClass, self).__init__()\n","        self.bert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier_level1 = torch.nn.Linear(768, num_labels_level1)  # Classifier for level 1\n","        self.classifier_level2 = torch.nn.Linear(768 + num_labels_level1, num_labels_level2)  # Classifier for level 2, input includes level 1 predictions\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        outputs = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n","        sequence_output, pooled_output = outputs[:2]\n","        pooled_output = self.dropout(pooled_output)\n","        \n","        level1_logits = self.classifier_level1(pooled_output)\n","        \n","        # Concatenate pooled_output with predictions from level 1 for level 2 input\n","        level2_input = torch.cat((pooled_output, level1_logits), 1)\n","        level2_logits = self.classifier_level2(level2_input)\n","        \n","        return level1_logits, level2_logits\n","\n","# Determine the number of unique labels for level 1 and level 2\n","num_labels_level1 = len(level1_cols)  # Replace with the actual number of level 1 labels\n","num_labels_level2 = len(level2_cols)  # Replace with the actual number of level 2 labels\n","\n","model = BERTClass(num_labels_level1, num_labels_level2)\n","model.to(device);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def loss_fn(outputs_level1, targets_level1, outputs_level2, targets_level2):\n","    loss_fct = torch.nn.BCEWithLogitsLoss()\n","    loss_level1 = loss_fct(outputs_level1.view(-1, num_labels_level1), targets_level1)\n","    loss_level2 = loss_fct(outputs_level2.view(-1, num_labels_level2), targets_level2)\n","    return loss_level1 + loss_level2  # You can also weigh these losses differently if needed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","\n","def train(epoch):\n","    model.train()\n","    total_loss = 0  # Track the total loss\n","    start_time = time.time()  # Capture the start time of the training\n","    \n","    for i, data in enumerate(train_loader):\n","        ids = data['ids'].to(device, dtype=torch.long)\n","        mask = data['mask'].to(device, dtype=torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","        targets_level1 = data['targets_level1'].to(device, dtype=torch.float)\n","        targets_level2 = data['targets_level2'].to(device, dtype=torch.float)\n","\n","        optimizer.zero_grad()\n","        level1_logits, level2_logits = model(ids, mask, token_type_ids)\n","\n","        loss = loss_fn(level1_logits, targets_level1, level2_logits, targets_level2)\n","        total_loss += loss.item()\n","        \n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % 50 == 0:\n","            elapsed_time = time.time() - start_time  # Calculate elapsed time\n","            print(f'Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}, Elapsed Time: {elapsed_time:.2f} seconds')\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    total_time = time.time() - start_time  # Total time for the epoch\n","    print(f'Epoch: {epoch}, Average Loss: {avg_loss}, Total Time: {total_time:.2f} seconds')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training loop\n","for epoch in range(EPOCHS):\n","    train(epoch)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'model.bin')"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"Fine-tuning BERT (and friends) for multi-label text classification.ipynb","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4399838,"sourceId":7554392,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"091f8220f33241f288faa0612853585f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6111a73e684a47769bda7183a836ee91","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b1899a0c4b144d7a5e6599f8afb8b65","value":3}},"1045bb16e3694410898a73cf1b848917":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbba60f793c14100934a268063f63d26","placeholder":"â","style":"IPY_MODEL_cd3570ddf67541d7818d97e236c54e54","value":" 3/3 [00:00&lt;00:00, 75.93it/s]"}},"308fa6a7348140ec981a8d6c7d31f346":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5080d322a8034924b652b379c04667ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6111a73e684a47769bda7183a836ee91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b1899a0c4b144d7a5e6599f8afb8b65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bc92587e35443488445e7521fbd0a13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5080d322a8034924b652b379c04667ed","placeholder":"â","style":"IPY_MODEL_cb95e545fbdd4e99903bf634df694c9f","value":"100%"}},"9a1aa9f2cc29473f9f8e5459d2641e76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bc92587e35443488445e7521fbd0a13","IPY_MODEL_091f8220f33241f288faa0612853585f","IPY_MODEL_1045bb16e3694410898a73cf1b848917"],"layout":"IPY_MODEL_308fa6a7348140ec981a8d6c7d31f346"}},"bbba60f793c14100934a268063f63d26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb95e545fbdd4e99903bf634df694c9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd3570ddf67541d7818d97e236c54e54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
